#!/usr/bin/env python3

import argparse
import json
import os
import re
import shutil
import tempfile
import uuid

"""
File Translation Utility

A tool for preparing files for translation, combining them for batch processing,
and restoring them afterward to their original locations.
"""

# Updated delimiter pattern with unique ID that won't be translated
FILE_DELIMITER = "###FILEID:{file_id}###FILENAME:{filename}###"
DELIMITER_PATTERN = r"###FILEID:([a-zA-Z0-9\-]+)###FILENAME:(.*?)###"


def prepare_files_for_translation(
    source_dir, extensions=(".md", ".mdoc", ".mdx", ".astro")
):
    """Prepares files for translation by converting to .txt format with unique identifiers."""
    output_dir = tempfile.mkdtemp(prefix="ots_translation_")
    # os.makedirs(output_dir, exist_ok=True) # mkdtemp already creates the directory
    file_mapping = {}

    for root, _, files in os.walk(source_dir):
        for filename in files:
            if filename.endswith(extensions):
                original_path = os.path.join(root, filename)
                relative_path = os.path.relpath(original_path, start=source_dir)

                # Generate unique name preserving path information
                directory = os.path.dirname(relative_path).replace(
                    os.path.sep, "-"
                )
                base_name, original_extension = os.path.splitext(
                    os.path.basename(relative_path)
                )

                # Create a temp name with path info
                if directory:
                    temp_name = f"{directory}-{base_name}.txt"
                else:
                    temp_name = f"{base_name}.txt"

                # Ensure uniqueness
                counter = 1
                final_temp_name = temp_name
                while final_temp_name in file_mapping:
                    final_temp_name = (
                        f"{os.path.splitext(temp_name)[0]}_{counter}.txt"
                    )
                    counter += 1

                # Generate unique ID for this file
                file_id = str(uuid.uuid4())[:8]

                translation_path = os.path.join(output_dir, final_temp_name)

                # Copy content
                shutil.copy2(original_path, translation_path)

                # Store mapping with original path, extension and ID
                file_mapping[final_temp_name] = {
                    "path": relative_path,
                    "extension": original_extension,
                    "original_full_path": original_path,
                    "file_id": file_id,
                }

    mapping_file = os.path.join(output_dir, "file_mapping.json")
    with open(mapping_file, "w") as f:
        json.dump(file_mapping, f, indent=2)

    print(
        f"Prepared {len(file_mapping)} files in temporary directory: {output_dir}"
    )
    # The calling function (main) will print the mapping_file path
    return mapping_file


def combine_txt_files(mapping_file_path, output_file=None):
    """Combines all .txt files using unique IDs in delimiters, based on a mapping file."""
    translation_dir = os.path.dirname(mapping_file_path)
    if output_file is None:
        output_file = os.path.join(translation_dir, "combined.txt")

    with open(mapping_file_path, "r") as f:
        file_mapping = json.load(f)

    combined_count = 0
    with open(output_file, "w", encoding="utf-8") as outfile:
        for filename, file_info in file_mapping.items():
            file_path = os.path.join(translation_dir, filename)
            if not os.path.exists(file_path):
                print(f"Warning: {file_path} not found, skipping")
                continue

            # Write file delimiter with ID and filename
            file_id = file_info.get("file_id")
            delimiter = FILE_DELIMITER.format(
                file_id=file_id, filename=filename
            )
            outfile.write(f"{delimiter}\n\n")

            # Write file content
            with open(file_path, "r", encoding="utf-8") as infile:
                outfile.write(infile.read())

            # Add spacing between files
            outfile.write("\n\n")
            combined_count += 1

    print(f"Combined {combined_count} files into {output_file}")
    return combined_count


def split_combined_file(mapping_file_path, combined_file=None):
    """Splits a combined file using unique IDs from delimiters, based on a mapping file."""
    translation_dir = os.path.dirname(mapping_file_path)
    if combined_file is None:
        combined_file = os.path.join(translation_dir, "combined.txt")

    os.makedirs(
        translation_dir, exist_ok=True
    )  # Ensures the directory for split files exists

    # First load the mapping to get file_id to filename correlations
    with open(mapping_file_path, "r") as f:
        file_mapping = json.load(f)

    # Build ID to filename mapping
    id_to_filename = {
        info["file_id"]: filename for filename, info in file_mapping.items()
    }

    with open(combined_file, "r", encoding="utf-8") as infile:
        content = infile.read()

    # Split the content based on file delimiters
    matches = re.finditer(DELIMITER_PATTERN, content)

    split_count = 0

    for match in matches:
        file_id = match.group(1)
        # We use the stored filename from our mapping rather than the one in the delimiter
        # which might have been translated
        if file_id in id_to_filename:
            filename = id_to_filename[file_id]
            print(f"Processing file: {file_id} {filename}")
            start_content = match.end()

            # Find the next delimiter or end of file
            next_match = re.search(DELIMITER_PATTERN, content[start_content:])
            if next_match:
                end_content = start_content + next_match.start()
            else:
                end_content = len(content)

            # Extract content between delimiters
            file_content = content[start_content:end_content].strip()

            # Write to file
            output_path = os.path.join(translation_dir, filename)
            with open(output_path, "w", encoding="utf-8") as outfile:
                outfile.write(file_content)

            split_count += 1
        else:
            print(f"Warning: File ID {file_id} not found in mapping")

    print(f"Split {split_count} files from {combined_file}")
    return split_count


def restore_translated_files(mapping_file_path):
    """Restores translated files to their original locations using the mapping file."""
    translation_dir = os.path.dirname(mapping_file_path)
    with open(mapping_file_path, "r") as f:
        file_mapping = json.load(f)

    restored_count = 0
    for temp_name, file_info in file_mapping.items():
        translated_file = os.path.join(translation_dir, temp_name)
        if not os.path.exists(translated_file):
            print(f"Warning: {translated_file} not found")
            continue

        # Get the original full path or construct it from source_dir and relative path
        if "original_full_path" in file_info:
            dest_path = file_info["original_full_path"]
        else:
            # This case should not happen if original_full_path is always populated.
            # If it does, it's an issue with the mapping file generation.
            print(
                f"Warning: 'original_full_path' not found for {temp_name}. Skipping."
            )
            continue

        # Ensure directory exists
        os.makedirs(os.path.dirname(dest_path), exist_ok=True)

        # Copy translated file to original location
        shutil.copy2(translated_file, dest_path)
        restored_count += 1

    print(f"Restored {restored_count} files to their original locations")


def main():
    parser = argparse.ArgumentParser(
        description="Translation utility for documentation file management",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""
Workflow Examples:

  1. Prepare files (outputs a mapping file path):
    ./translation-pluribus-util prepare ./src/content/docs/en
    => Prepared X files in temporary directory: /tmp/ots_translation_xxxx
    => Process complete. Mapping file is at: /tmp/ots_translation_xxxx/file_mapping.json
    (Use the printed mapping file path for subsequent commands)

2. Combine files (using the mapping file path from step 1):
    ./translation-pluribus-util combine /tmp/ots_translation_xxxx/file_mapping.json [./path/to/combined_output.txt]
    (If output file is omitted, defaults to 'combined.txt' in the same directory as the mapping file)

3. Split translated file (after translation, using the same mapping file path):
    ./translation-pluribus-util split /tmp/ots_translation_xxxx/file_mapping.json [./path/to/translated_combined.txt]
    (If combined file is omitted, defaults to 'combined.txt' in the same directory as the mapping file)

4. Restore files (using the same mapping file path):
    ./translation-pluribus-util restore /tmp/ots_translation_xxxx/file_mapping.json
      ""\",
  )
""",
    )

    subparsers = parser.add_subparsers(
        dest="command", help="Command to execute", required=True
    )

    # Prepare command
    prepare_parser = subparsers.add_parser(
        "prepare", help="Prepare files for translation"
    )
    prepare_parser.add_argument(
        "source_dir",
        help="Source directory for documents (e.g., ./src/content/docs/en)",
    )
    # Removed translation_dir argument, will use a temporary directory
    prepare_parser.add_argument(
        "-e",
        "--extensions",
        dest="extensions",
        default=".md,.mdoc,.mdx,.astro",
        help="Comma-separated list of file extensions to process",
    )

    # Combine command
    combine_parser = subparsers.add_parser(
        "combine", help="Combine files for translation"
    )
    combine_parser.add_argument(
        "mapping_file_path",
        help="Path to the file_mapping.json file",
    )
    combine_parser.add_argument(
        "combined_file",
        nargs="?",
        default=None,
        help="Path for combined output file (e.g., ./combined_en.txt). Defaults to 'combined.txt' in mapping file's directory.",
    )

    # Split command
    split_parser = subparsers.add_parser(
        "split", help="Split combined translated file"
    )
    split_parser.add_argument(
        "mapping_file_path",  # mapping_file_path is now the first positional argument
        help="Path to the file_mapping.json file",
    )
    split_parser.add_argument(
        "combined_file",
        nargs="?",
        default=None,
        help="Path to the combined translated file (e.g., ./translated_combined_en.txt). Defaults to 'combined.txt' in mapping file's directory.",
    )

    # Restore command
    restore_parser = subparsers.add_parser(
        "restore", help="Restore translated files"
    )
    restore_parser.add_argument(
        "mapping_file_path",
        help="Path to the file_mapping.json file",
    )
    # source_dir argument removed, as original_full_path from mapping file is used

    args = parser.parse_args()

    if args.command == "prepare":
        extensions = tuple(args.extensions.split(","))
        mapping_file_path = prepare_files_for_translation(
            args.source_dir, extensions
        )
        print(f"Process complete. Mapping file is at: {mapping_file_path}")
        print(f"""
Use this path for subsequent combine, split, or restore commands.

i.e. pnpm run translate:combine {mapping_file_path}
        """)
    elif args.command == "combine":
        combine_txt_files(args.mapping_file_path, args.combined_file)
    elif args.command == "split":
        # For split, mapping_file_path is first, then optional combined_file
        split_combined_file(args.mapping_file_path, args.combined_file)
    elif args.command == "restore":
        restore_translated_files(args.mapping_file_path)


if __name__ == "__main__":
    main()
